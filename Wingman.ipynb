{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kennyy127/AI-AOL/blob/main/Wingman.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtTr_uOG6GVY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Bidirectional, Concatenate, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EHc_RgN6yuU"
      },
      "outputs": [],
      "source": [
        "def load_datasets():\n",
        "    conversational_dataset = pd.read_csv('conversational_dataset_25k.csv')\n",
        "    sentiment_dataset = pd.read_csv('sentiment_analysis.csv')\n",
        "    cornell_dataset = pd.read_csv('Cornell_Movie_Dialog_Structured_Dataset.csv')\n",
        "    synthetic_train = pd.read_csv('Synthetic-Persona-Chat_train.csv')\n",
        "    synthetic_valid = pd.read_csv('Synthetic-Persona-Chat_valid.csv')\n",
        "    synthetic_test = pd.read_csv('Synthetic-Persona-Chat_test.csv')\n",
        "\n",
        "    synthetic_persona_chat = pd.concat([synthetic_train, synthetic_valid, synthetic_test], ignore_index=True)\n",
        "    synthetic_persona_chat = synthetic_persona_chat.rename(columns={'context': 'text', 'response': 'response'})\n",
        "\n",
        "    return conversational_dataset, sentiment_dataset, cornell_dataset, synthetic_persona_chat\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_and_convert_to_string(dataset):\n",
        "    for column in ['text', 'response']:\n",
        "        if column in dataset.columns:\n",
        "            dataset[column] = dataset[column].fillna('')\n",
        "            dataset[column] = dataset[column].astype(str)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "Ih8CLEY7g8em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJyf1DRz66L5"
      },
      "outputs": [],
      "source": [
        "def preprocess_datasets(conversational_dataset, sentiment_dataset, cornell_dataset, synthetic_persona_chat):\n",
        "    conversational_dataset = conversational_dataset.rename(columns={'Input': 'text', 'Response': 'response'})\n",
        "\n",
        "    if 'text' in sentiment_dataset.columns and 'sentiment' in sentiment_dataset.columns:\n",
        "        sentiment_dataset = sentiment_dataset.rename(columns={'text': 'text', 'sentiment': 'response'})\n",
        "    else:\n",
        "        raise KeyError(\"Sentiment dataset is missing required columns 'text' and 'sentiment'.\")\n",
        "\n",
        "    if 'text' in cornell_dataset.columns and 'label' in cornell_dataset.columns:\n",
        "        cornell_dataset = cornell_dataset.rename(columns={'text': 'text', 'label': 'response'})\n",
        "    else:\n",
        "        raise KeyError(\"Cornell dataset is missing required columns 'text' and 'label'.\")\n",
        "\n",
        "    if 'Best Generated Conversation' in synthetic_persona_chat.columns:\n",
        "        synthetic_persona_chat = synthetic_persona_chat.rename(\n",
        "            columns={'user 1 personas': 'text', 'Best Generated Conversation': 'response'}\n",
        "        )\n",
        "    else:\n",
        "        raise KeyError(\"Synthetic Persona Chat dataset is missing required columns.\")\n",
        "\n",
        "    # Clean datasets\n",
        "    conversational_dataset = clean_and_convert_to_string(conversational_dataset)\n",
        "    sentiment_dataset = clean_and_convert_to_string(sentiment_dataset)\n",
        "    cornell_dataset = clean_and_convert_to_string(cornell_dataset)\n",
        "    synthetic_persona_chat = clean_and_convert_to_string(synthetic_persona_chat)\n",
        "\n",
        "    # Combine datasets\n",
        "    combined_dataset = pd.concat([\n",
        "        conversational_dataset[['text', 'response']],\n",
        "        sentiment_dataset[['text', 'response']],\n",
        "        cornell_dataset[['text', 'response']],\n",
        "        synthetic_persona_chat[['text', 'response']]\n",
        "    ], ignore_index=True)\n",
        "\n",
        "    # Shuffle the dataset\n",
        "    combined_dataset = shuffle(combined_dataset, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    return combined_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkXEVQSv8mGO"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def tokenize_and_pad(dataset, vocab_size=10000, max_length=50):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "    tokenizer.fit_on_texts(dataset['text'].values)\n",
        "\n",
        "    text_sequences = tokenizer.texts_to_sequences(dataset['text'].values)\n",
        "    response_sequences = tokenizer.texts_to_sequences(dataset['response'].values)\n",
        "\n",
        "    text_padded = pad_sequences(text_sequences, maxlen=max_length, padding='post')  # Use text_sequences\n",
        "    response_padded = pad_sequences(response_sequences, maxlen=max_length, padding='post')  # Use response_sequences\n",
        "\n",
        "    decoder_input = response_padded[:, :-1]\n",
        "    decoder_target = response_padded[:, 1:]\n",
        "\n",
        "    return text_padded, decoder_input, decoder_target, tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6Lj3HLLra3X"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "\n",
        "def build_model(vocab_size, input_length):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(input_length,), name='encoder_inputs')\n",
        "    encoder_embedding = Embedding(input_dim=vocab_size, output_dim=128, input_length=input_length)(encoder_inputs)\n",
        "    encoder_lstm, state_h, state_c = LSTM(128, return_state=True, name='encoder_lstm')(encoder_embedding)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(input_length - 1,), name='decoder_inputs')\n",
        "    decoder_embedding = Embedding(input_dim=vocab_size, output_dim=128, input_length=input_length - 1)(decoder_inputs)\n",
        "    decoder_lstm = LSTM(128, return_sequences=True, return_state=False, name='decoder_lstm')\n",
        "    decoder_outputs = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "    decoder_dense = Dense(vocab_size, activation='softmax', name='decoder_dense')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # Model\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs, name='seq2seq_model')\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TVdCfVW1obH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbd86e57-e2d1-4e29-af52-bd06a7aecda2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4863/4863\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 32ms/step - accuracy: 0.9189 - loss: 0.6620 - val_accuracy: 0.9669 - val_loss: 0.1547\n",
            "Epoch 2/10\n",
            "\u001b[1m4863/4863\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 32ms/step - accuracy: 0.9680 - loss: 0.1459 - val_accuracy: 0.9701 - val_loss: 0.1315\n",
            "Epoch 3/10\n",
            "\u001b[1m4863/4863\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 32ms/step - accuracy: 0.9708 - loss: 0.1248 - val_accuracy: 0.9718 - val_loss: 0.1218\n",
            "Epoch 4/10\n",
            "\u001b[1m4863/4863\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 32ms/step - accuracy: 0.9730 - loss: 0.1117 - val_accuracy: 0.9728 - val_loss: 0.1168\n",
            "Epoch 5/10\n",
            "\u001b[1m4863/4863\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 32ms/step - accuracy: 0.9736 - loss: 0.1062 - val_accuracy: 0.9734 - val_loss: 0.1138\n",
            "Epoch 6/10\n",
            "\u001b[1m4863/4863\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 32ms/step - accuracy: 0.9743 - loss: 0.1018 - val_accuracy: 0.9741 - val_loss: 0.1119\n",
            "Epoch 7/10\n",
            "\u001b[1m4863/4863\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 32ms/step - accuracy: 0.9761 - loss: 0.0958 - val_accuracy: 0.9756 - val_loss: 0.1090\n",
            "Epoch 8/10\n",
            "\u001b[1m4863/4863\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 32ms/step - accuracy: 0.9772 - loss: 0.0908 - val_accuracy: 0.9757 - val_loss: 0.1084\n",
            "Epoch 9/10\n",
            "\u001b[1m4863/4863\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 32ms/step - accuracy: 0.9784 - loss: 0.0854 - val_accuracy: 0.9758 - val_loss: 0.1082\n",
            "Epoch 10/10\n",
            "\u001b[1m4863/4863\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 32ms/step - accuracy: 0.9780 - loss: 0.0857 - val_accuracy: 0.9759 - val_loss: 0.1084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:The `save_format` argument is deprecated in Keras 3. We recommend removing this argument as it can be inferred from the file path. Received: save_format=keras\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training complete and saved at 'Wingman.keras'\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    datasets = load_datasets()\n",
        "    combined_dataset = preprocess_datasets(*datasets)\n",
        "\n",
        "    text_padded, decoder_input, decoder_target, tokenizer = tokenize_and_pad(combined_dataset)\n",
        "\n",
        "    model = build_model(vocab_size=10000, input_length=50)\n",
        "\n",
        "\n",
        "    history = model.fit(\n",
        "        [text_padded, decoder_input],\n",
        "        decoder_target,\n",
        "        batch_size=32,\n",
        "        epochs=10,\n",
        "        validation_split=0.2\n",
        ")\n",
        "\n",
        "    model.save(\"Wingman.keras\", save_format=\"keras\")\n",
        "    print(\"Model training complete and saved at 'Wingman.keras'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(model, tokenizer, max_input_length=50, max_response_length=49):\n",
        "    print(\"Type 'exit' to end the chat.\")\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        user_sequence = tokenizer.texts_to_sequences([user_input])\n",
        "        user_padded = pad_sequences(user_sequence, maxlen=max_input_length, padding='post')\n",
        "\n",
        "        decoder_input = np.zeros((1, max_response_length))\n",
        "\n",
        "        response = \"\"\n",
        "        for i in range(max_response_length):\n",
        "            prediction = model.predict([user_padded, decoder_input], verbose=0)\n",
        "            print(f\"Step {i}: Prediction array = {prediction[0]}\")\n",
        "\n",
        "            next_token = np.argmax(prediction[0])\n",
        "            print(f\"Step {i}: Next token = {next_token}\")\n",
        "\n",
        "            if next_token == tokenizer.word_index.get('<end>', None):\n",
        "                break\n",
        "\n",
        "            predicted_word = tokenizer.index_word.get(next_token, '<unknown>')\n",
        "            if predicted_word == '<unknown>':\n",
        "                print(f\"Unknown token: {next_token}\")\n",
        "                break\n",
        "\n",
        "            response += predicted_word + ' '\n",
        "\n",
        "            if i < max_response_length - 1:\n",
        "                decoder_input[0, i + 1] = next_token\n",
        "\n",
        "        print(\"Bot:\", response.strip())\n",
        "\n",
        "\n",
        "chat(model, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "moZ3JlIZY1cy",
        "outputId": "3945253b-1cd6-45be-cd6f-362d47e55505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type 'exit' to end the chat.\n",
            "You: hi\n",
            "Step 0: Prediction array = [[1.0000000e+00 8.6693266e-09 1.4099787e-11 ... 1.6321144e-14\n",
            "  1.8169321e-14 1.8641820e-14]\n",
            " [1.0000000e+00 4.2315829e-10 7.4147476e-12 ... 7.5480258e-15\n",
            "  8.3583128e-15 8.6155135e-15]\n",
            " [1.0000000e+00 4.0426440e-10 7.9556830e-12 ... 4.6547650e-15\n",
            "  5.2591090e-15 5.4114165e-15]\n",
            " ...\n",
            " [1.0000000e+00 2.2388133e-12 1.3808893e-14 ... 1.6813026e-16\n",
            "  1.8920564e-16 1.8565865e-16]\n",
            " [1.0000000e+00 2.2464487e-12 1.3608141e-14 ... 1.6852967e-16\n",
            "  1.8967175e-16 1.8604786e-16]\n",
            " [1.0000000e+00 2.2539466e-12 1.3409923e-14 ... 1.6889780e-16\n",
            "  1.9010130e-16 1.8640662e-16]]\n",
            "Step 0: Next token = 0\n",
            "Unknown token: 0\n",
            "Bot: \n",
            "You: how are you\n",
            "Step 0: Prediction array = [[9.99999881e-01 2.13630305e-08 8.18170184e-11 ... 3.92766386e-13\n",
            "  4.82460805e-13 4.70225963e-13]\n",
            " [1.00000000e+00 8.18560830e-10 5.68574571e-12 ... 4.78186826e-14\n",
            "  5.45396587e-14 5.43599894e-14]\n",
            " [1.00000000e+00 9.47155909e-10 2.82863034e-12 ... 2.55474048e-14\n",
            "  2.85646242e-14 2.87036765e-14]\n",
            " ...\n",
            " [1.00000000e+00 1.48448907e-12 1.94812767e-14 ... 1.04034281e-16\n",
            "  1.16589347e-16 1.17658514e-16]\n",
            " [1.00000000e+00 1.50277859e-12 1.95721887e-14 ... 1.06417725e-16\n",
            "  1.19329141e-16 1.20273305e-16]\n",
            " [1.00000000e+00 1.52361956e-12 1.96898721e-14 ... 1.08962960e-16\n",
            "  1.22260114e-16 1.23075250e-16]]\n",
            "Step 0: Next token = 10000\n",
            "Step 1: Prediction array = [[9.9999988e-01 2.1363030e-08 8.1817018e-11 ... 3.9276639e-13\n",
            "  4.8246081e-13 4.7022596e-13]\n",
            " [9.9929750e-01 1.3904726e-05 3.4989764e-05 ... 2.0650082e-09\n",
            "  2.1434470e-09 2.2434998e-09]\n",
            " [1.0000000e+00 3.0849359e-09 1.4246197e-11 ... 5.6730015e-14\n",
            "  6.4855463e-14 6.6299315e-14]\n",
            " ...\n",
            " [1.0000000e+00 1.4023530e-12 1.6926305e-14 ... 1.1122657e-16\n",
            "  1.2546680e-16 1.2551611e-16]\n",
            " [1.0000000e+00 1.4387008e-12 1.7210539e-14 ... 1.1459454e-16\n",
            "  1.2933602e-16 1.2925365e-16]\n",
            " [1.0000000e+00 1.4766272e-12 1.7508127e-14 ... 1.1805459e-16\n",
            "  1.3330574e-16 1.3308624e-16]]\n",
            "Step 1: Next token = 20000\n",
            "Step 2: Prediction array = [[9.9999988e-01 2.1363030e-08 8.1817018e-11 ... 3.9276639e-13\n",
            "  4.8246081e-13 4.7022596e-13]\n",
            " [9.9929750e-01 1.3904726e-05 3.4989764e-05 ... 2.0650082e-09\n",
            "  2.1434470e-09 2.2434998e-09]\n",
            " [9.9124449e-01 1.6907034e-05 2.7650363e-05 ... 3.7564445e-09\n",
            "  3.6910468e-09 3.9447929e-09]\n",
            " ...\n",
            " [1.0000000e+00 1.2261814e-12 1.5224763e-14 ... 1.0557145e-16\n",
            "  1.1954191e-16 1.1925587e-16]\n",
            " [1.0000000e+00 1.2604030e-12 1.5472395e-14 ... 1.0869477e-16\n",
            "  1.2312504e-16 1.2272270e-16]\n",
            " [1.0000000e+00 1.2977143e-12 1.5750020e-14 ... 1.1201087e-16\n",
            "  1.2692397e-16 1.2639925e-16]]\n",
            "Step 2: Next token = 30000\n",
            "Step 3: Prediction array = [[9.9999988e-01 2.1363030e-08 8.1817018e-11 ... 3.9276639e-13\n",
            "  4.8246081e-13 4.7022596e-13]\n",
            " [9.9929750e-01 1.3904726e-05 3.4989764e-05 ... 2.0650082e-09\n",
            "  2.1434470e-09 2.2434998e-09]\n",
            " [9.9124449e-01 1.6907034e-05 2.7650363e-05 ... 3.7564445e-09\n",
            "  3.6910468e-09 3.9447929e-09]\n",
            " ...\n",
            " [1.0000000e+00 1.1899845e-12 1.4686823e-14 ... 1.0479386e-16\n",
            "  1.1879366e-16 1.1839690e-16]\n",
            " [1.0000000e+00 1.2258774e-12 1.4958908e-14 ... 1.0800123e-16\n",
            "  1.2246922e-16 1.2195827e-16]\n",
            " [1.0000000e+00 1.2649186e-12 1.5261252e-14 ... 1.1140958e-16\n",
            "  1.2637033e-16 1.2573751e-16]]\n",
            "Step 3: Next token = 40000\n",
            "Step 4: Prediction array = [[9.99999881e-01 2.13630305e-08 8.18170184e-11 ... 3.92766386e-13\n",
            "  4.82460805e-13 4.70225963e-13]\n",
            " [9.99297500e-01 1.39047261e-05 3.49897637e-05 ... 2.06500816e-09\n",
            "  2.14344698e-09 2.24349983e-09]\n",
            " [9.91244495e-01 1.69070336e-05 2.76503633e-05 ... 3.75644449e-09\n",
            "  3.69104680e-09 3.94479294e-09]\n",
            " ...\n",
            " [1.00000000e+00 1.15801476e-12 1.33422885e-14 ... 1.07867409e-16\n",
            "  1.22601044e-16 1.21738871e-16]\n",
            " [1.00000000e+00 1.19329600e-12 1.34730170e-14 ... 1.11069683e-16\n",
            "  1.26238786e-16 1.25251609e-16]\n",
            " [1.00000000e+00 1.22700483e-12 1.35111293e-14 ... 1.14148648e-16\n",
            "  1.29722911e-16 1.28603954e-16]]\n",
            "Step 4: Next token = 60000\n",
            "Unknown token: 60000\n",
            "Bot: mayhem homing shouldn't've buddy'd\n",
            "You: whats your name\n",
            "Step 0: Prediction array = [[1.0000000e+00 4.2877197e-08 2.1920514e-11 ... 8.2340723e-14\n",
            "  9.4869086e-14 9.5528572e-14]\n",
            " [1.0000000e+00 3.3126385e-10 1.9848450e-12 ... 3.5555335e-15\n",
            "  4.0095063e-15 4.1170871e-15]\n",
            " [1.0000000e+00 2.0480236e-11 1.5306911e-13 ... 4.9012407e-16\n",
            "  5.5856402e-16 5.7065510e-16]\n",
            " ...\n",
            " [1.0000000e+00 2.3445922e-12 9.5606532e-15 ... 1.7345273e-16\n",
            "  1.9594506e-16 1.9086572e-16]\n",
            " [1.0000000e+00 2.3439751e-12 9.5564239e-15 ... 1.7343818e-16\n",
            "  1.9593234e-16 1.9085262e-16]\n",
            " [1.0000000e+00 2.3434566e-12 9.5516865e-15 ... 1.7342892e-16\n",
            "  1.9592338e-16 1.9084241e-16]]\n",
            "Step 0: Next token = 0\n",
            "Unknown token: 0\n",
            "Bot: \n",
            "You: what's your name\n",
            "Step 0: Prediction array = [[9.9999881e-01 1.1576924e-06 1.5729049e-10 ... 9.0756436e-14\n",
            "  1.1002475e-13 1.0513904e-13]\n",
            " [1.0000000e+00 9.0315062e-09 1.2899242e-11 ... 1.0951688e-14\n",
            "  1.2562976e-14 1.2332447e-14]\n",
            " [1.0000000e+00 8.0891327e-10 4.0236777e-12 ... 4.8821539e-15\n",
            "  5.4342131e-15 5.5259413e-15]\n",
            " ...\n",
            " [1.0000000e+00 2.1494553e-12 2.5006391e-14 ... 1.4594043e-16\n",
            "  1.6386882e-16 1.6430634e-16]\n",
            " [1.0000000e+00 2.1692992e-12 2.5059343e-14 ... 1.4807759e-16\n",
            "  1.6628816e-16 1.6660691e-16]\n",
            " [1.0000000e+00 2.1900947e-12 2.5131620e-14 ... 1.5011713e-16\n",
            "  1.6860233e-16 1.6881150e-16]]\n",
            "Step 0: Next token = 10000\n",
            "Step 1: Prediction array = [[9.9999881e-01 1.1576924e-06 1.5729049e-10 ... 9.0756436e-14\n",
            "  1.1002475e-13 1.0513904e-13]\n",
            " [8.8740391e-01 3.9270227e-03 8.3871251e-03 ... 2.9898306e-08\n",
            "  3.2676425e-08 3.3598909e-08]\n",
            " [1.0000000e+00 2.0810125e-08 1.4753133e-11 ... 2.6824873e-14\n",
            "  3.0582656e-14 3.0775641e-14]\n",
            " ...\n",
            " [1.0000000e+00 2.0526371e-12 2.3388414e-14 ... 1.4824771e-16\n",
            "  1.6696447e-16 1.6671754e-16]\n",
            " [1.0000000e+00 2.0758033e-12 2.3539505e-14 ... 1.5059490e-16\n",
            "  1.6965137e-16 1.6927514e-16]\n",
            " [1.0000000e+00 2.0978540e-12 2.3683028e-14 ... 1.5279964e-16\n",
            "  1.7217712e-16 1.7167735e-16]]\n",
            "Step 1: Next token = 20000\n",
            "Step 2: Prediction array = [[9.9999881e-01 1.1576924e-06 1.5729049e-10 ... 9.0756436e-14\n",
            "  1.1002475e-13 1.0513904e-13]\n",
            " [8.8740391e-01 3.9270227e-03 8.3871251e-03 ... 2.9898306e-08\n",
            "  3.2676425e-08 3.3598909e-08]\n",
            " [9.9515450e-01 4.4528941e-05 2.9240024e-05 ... 3.9995691e-09\n",
            "  3.9557633e-09 4.0969703e-09]\n",
            " ...\n",
            " [1.0000000e+00 1.7087734e-12 1.8011359e-14 ... 1.4677007e-16\n",
            "  1.6655417e-16 1.6473625e-16]\n",
            " [1.0000000e+00 1.7350239e-12 1.7935050e-14 ... 1.4855964e-16\n",
            "  1.6850331e-16 1.6661644e-16]\n",
            " [1.0000000e+00 1.7583376e-12 1.7730361e-14 ... 1.5003757e-16\n",
            "  1.7008877e-16 1.6812899e-16]]\n",
            "Step 2: Next token = 40000\n",
            "Step 3: Prediction array = [[9.9999881e-01 1.1576924e-06 1.5729049e-10 ... 9.0756436e-14\n",
            "  1.1002475e-13 1.0513904e-13]\n",
            " [8.8740391e-01 3.9270227e-03 8.3871251e-03 ... 2.9898306e-08\n",
            "  3.2676425e-08 3.3598909e-08]\n",
            " [9.9515450e-01 4.4528941e-05 2.9240024e-05 ... 3.9995691e-09\n",
            "  3.9557633e-09 4.0969703e-09]\n",
            " ...\n",
            " [1.0000000e+00 1.7843826e-12 1.0603530e-14 ... 1.4619453e-16\n",
            "  1.6547314e-16 1.6237848e-16]\n",
            " [1.0000000e+00 1.8798730e-12 1.0410188e-14 ... 1.5112665e-16\n",
            "  1.7101001e-16 1.6756041e-16]\n",
            " [1.0000000e+00 1.9604789e-12 1.0252156e-14 ... 1.5524200e-16\n",
            "  1.7561186e-16 1.7189033e-16]]\n",
            "Step 3: Next token = 60000\n",
            "Unknown token: 60000\n",
            "Bot: mayhem homing buddy'd\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-ec4a7f9b8fa5>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Call the chat function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mchat_with_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-54-ec4a7f9b8fa5>\u001b[0m in \u001b[0;36mchat_with_model\u001b[0;34m(model, tokenizer, max_input_length, max_response_length)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Type 'exit' to end the chat.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Goodbye!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2lMtGm6Ufhd",
        "outputId": "5f6e483d-0fbe-4c47-ea22-65c455ab959d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: gh: command not found\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1E82OR9my8lil-DyqqrDtlOuFVe7i5Qhh",
      "authorship_tag": "ABX9TyPzQkRiLJmtOedccJU1XU7/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}